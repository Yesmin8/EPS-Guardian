#!/usr/bin/env python3
"""
CONVERTISSEUR DE MOD√àLE IA OBC ‚Üí MCU
Convertit un mod√®le TensorFlow (.h5) en format TensorFlow Lite (.tflite)
et pr√©pare les fichiers pour d√©ploiement sur ESP32.
"""

import os
import tensorflow as tf
import numpy as np
import json
import logging
from datetime import datetime

# --- Configuration des chemins ---
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "..", ".."))
MODEL_DIR = os.path.join(PROJECT_ROOT, "data", "ai_models", "model_complex")
DEPLOY_DIR = os.path.join(PROJECT_ROOT, "data", "ai_models", "esp32_deployment")
MCU_MODEL_DIR = os.path.join(PROJECT_ROOT, "data", "ai_models", "model_simple")

os.makedirs(DEPLOY_DIR, exist_ok=True)
os.makedirs(MCU_MODEL_DIR, exist_ok=True)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("AI_Model_Converter")

class AIModelConverter:
    def __init__(self):
        self.model = None
        self.thresholds = None
        self.model_info = {}
        
    def load_complex_model(self):
        """Charge le mod√®le LSTM complexe et ses m√©tadonn√©es"""
        try:
            model_path = os.path.join(MODEL_DIR, "ai_model_lstm_autoencoder.h5")
            thresholds_path = os.path.join(MODEL_DIR, "ai_thresholds.json")
            
            if not os.path.exists(model_path):
                logger.error(f"Mod√®le non trouv√©: {model_path}")
                return False
                
            logger.info(f"Chargement du mod√®le: {model_path}")
            self.model = tf.keras.models.load_model(model_path, compile=False)
            
            # Chargement des seuils
            if os.path.exists(thresholds_path):
                with open(thresholds_path, 'r') as f:
                    thresholds_data = json.load(f)
                    self.thresholds = thresholds_data["anomaly_thresholds"]
                logger.info("Seuils d'anomalie charg√©s")
            else:
                logger.warning("Fichier de seuils non trouv√©, utilisation de valeurs par d√©faut")
                self.thresholds = {
                    "normal_threshold": 0.001,
                    "warning_threshold": 0.002, 
                    "critical_threshold": 0.003
                }
            
            # Informations du mod√®le
            self.model_info = {
                "input_shape": self.model.input_shape[1:],
                "output_shape": self.model.output_shape[1:],
                "total_params": self.model.count_params(),
                "model_size_h5": os.path.getsize(model_path) / 1024
            }
            
            logger.info(f"Mod√®le charg√©: {self.model_info['input_shape']} ‚Üí {self.model_info['output_shape']}")
            logger.info(f"Param√®tres: {self.model_info['total_params']:,}")
            logger.info(f"Taille .h5: {self.model_info['model_size_h5']:.1f} KB")
            
            return True
            
        except Exception as e:
            logger.error(f"Erreur chargement mod√®le: {e}")
            return False
    
    def convert_to_tflite(self, output_path, quantize=False, quantization_type="float16"):
        """Convertit le mod√®le en TensorFlow Lite avec support des LSTM dynamiques"""
        try:
            logger.info(f"Conversion en TFLite: quantize={quantize}, type={quantization_type}")

            # --- Cr√©ation du convertisseur ---
            converter = tf.lite.TFLiteConverter.from_keras_model(self.model)

            # üîß Correction pour les LSTM (Select TF Ops)
            converter.target_spec.supported_ops = [
                tf.lite.OpsSet.TFLITE_BUILTINS,
                tf.lite.OpsSet.SELECT_TF_OPS
            ]
            converter._experimental_lower_tensor_list_ops = False  # indispensable pour TensorListReserve

            # --- Gestion de la quantification ---
            if quantize:
                if quantization_type == "float16":
                    converter.optimizations = [tf.lite.Optimize.DEFAULT]
                    converter.target_spec.supported_types = [tf.float16]
                    logger.info("Quantification float16 activ√©e")
                elif quantization_type == "int8":
                    converter.optimizations = [tf.lite.Optimize.DEFAULT]

                    def representative_dataset():
                        for _ in range(100):
                            data = np.random.randn(1, *self.model_info['input_shape']).astype(np.float32)
                            yield [data]

                    converter.representative_dataset = representative_dataset
                    converter.target_spec.supported_ops = [
                        tf.lite.OpsSet.TFLITE_BUILTINS_INT8,
                        tf.lite.OpsSet.SELECT_TF_OPS
                    ]
                    converter.inference_input_type = tf.int8
                    converter.inference_output_type = tf.int8
                    logger.info("Quantification INT8 activ√©e")

            # --- Conversion ---
            tflite_model = converter.convert()

            # --- Sauvegarde ---
            with open(output_path, "wb") as f:
                f.write(tflite_model)

            size_kb = os.path.getsize(output_path) / 1024
            logger.info(f"Mod√®le TFLite g√©n√©r√©: {output_path}")
            logger.info(f"Taille TFLite: {size_kb:.1f} KB")

            return size_kb

        except Exception as e:
            logger.error(f"Erreur conversion TFLite: {e}")
            return 0
    
    def generate_c_header(self, tflite_path, header_path):
        """G√©n√®re un fichier header C/C++ √† partir du mod√®le TFLite"""
        try:
            logger.info(f"G√©n√©ration du header C: {header_path}")
            
            with open(tflite_path, "rb") as f:
                model_data = f.read()
            
            # Conversion en tableau C
            hex_array = []
            for i, byte in enumerate(model_data):
                if i % 12 == 0:
                    hex_array.append("\n    ")
                hex_array.append(f"0x{byte:02x}, ")
            
            hex_string = "".join(hex_array).rstrip(", ")
            
            header_content = f"""// ====================================================
// Fichier g√©n√©r√© automatiquement - EPS Guardian AI Model
// Mod√®le TensorFlow Lite pour d√©ploiement ESP32
// G√©n√©r√© le: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
// ====================================================

#ifndef EPS_GUARDIAN_AI_MODEL_H
#define EPS_GUARDIAN_AI_MODEL_H

#include <cstdint>
#include <cstddef>

namespace eps_guardian {{
namespace ai_model {{

// Donn√©es du mod√®le TensorFlow Lite
alignas(8) const uint8_t g_ai_model_data[] = {{
    {hex_string}
}};

// Taille du mod√®le en bytes
const size_t g_ai_model_size = sizeof(g_ai_model_data);

// Seuils d'anomalie (bas√©s sur l'erreur de reconstruction)
constexpr float NORMAL_THRESHOLD = {self.thresholds['normal_threshold']:.6f}f;
constexpr float WARNING_THRESHOLD = {self.thresholds['warning_threshold']:.6f}f;
constexpr float CRITICAL_THRESHOLD = {self.thresholds['critical_threshold']:.6f}f;

// Configuration du mod√®le
constexpr int SEQUENCE_LENGTH = {self.model_info['input_shape'][0]};
constexpr int FEATURE_COUNT = {self.model_info['input_shape'][1]};
constexpr int TENSOR_ARENA_SIZE = 40 * 1024; // 40KB pour LSTM

}} // namespace ai_model
}} // namespace eps_guardian

#endif // EPS_GUARDIAN_AI_MODEL_H
"""
            
            with open(header_path, "w") as f:
                f.write(header_content)
            
            logger.info(f"Header C g√©n√©r√©: {header_path}")
            return True
            
        except Exception as e:
            logger.error(f"Erreur g√©n√©ration header: {e}")
            return False
    
    def generate_arduino_sketch(self, sketch_path):
        """G√©n√®re un sketch Arduino complet pour l'ESP32"""
        try:
            logger.info(f"G√©n√©ration du sketch Arduino: {sketch_path}")
            
            sketch_content = f"""// ====================================================
// EPS Guardian - Syst√®me de D√©tection d'Anomalies IA
// MCU: ESP32 avec TensorFlow Lite Micro
// Mod√®le: LSTM Autoencoder pour s√©quences temporelles
// ====================================================

#include "eps_guardian_ai_model.h"
#include <TensorFlowLite.h>
#include <tensorflow/lite/micro/all_ops_resolver.h>
#include <tensorflow/lite/micro/micro_interpreter.h>
#include <tensorflow/lite/schema/schema_generated.h>

// ====================================================
// CONFIGURATION MAT√âRIELLE
// ====================================================

// Broches pour LEDs de statut
#define LED_NORMAL 2
#define LED_WARNING 4  
#define LED_CRITICAL 5

// Buffer pour l'inf√©rence
constexpr int kTensorArenaSize = eps_guardian::ai_model::TENSOR_ARENA_SIZE;
static uint8_t tensor_arena[kTensorArenaSize];

// ====================================================
// CLASSE PRINCIPALE EPS GUARDIAN AI
// ====================================================

class EPSGuardianAI {{
private:
    const tflite::Model* model;
    tflite::MicroInterpreter* interpreter;
    TfLiteTensor* input;
    TfLiteTensor* output;
    tflite::AllOpsResolver resolver;

    // Buffer pour les s√©quences capteurs
    float sensor_sequence[eps_guardian::ai_model::SEQUENCE_LENGTH]
                        [eps_guardian::ai_model::FEATURE_COUNT];
    int sequence_index = 0;

public:
    bool initialize() {{
        Serial.println("Initialisation EPS Guardian AI...");
        
        // Chargement du mod√®le
        model = tflite::GetModel(eps_guardian::ai_model::g_ai_model_data);
        if (model->version() != TFLITE_SCHEMA_VERSION) {{
            Serial.println("ERREUR: Version du mod√®le incompatible!");
            return false;
        }}
        
        // Initialisation de l'interpr√©teur
        static tflite::MicroInterpreter static_interpreter(
            model, resolver, tensor_arena, kTensorArenaSize);
        interpreter = &static_interpreter;
        
        // Allocation des tenseurs
        if (interpreter->AllocateTensors() != kTfLiteOk) {{
            Serial.println("ERREUR: Allocation des tenseurs √©chou√©e!");
            return false;
        }}
        
        input = interpreter->input(0);
        output = interpreter->output(0);
        
        // Configuration des broches
        pinMode(LED_NORMAL, OUTPUT);
        pinMode(LED_WARNING, OUTPUT);  
        pinMode(LED_CRITICAL, OUTPUT);
        
        Serial.println("EPS Guardian AI initialis√© avec succ√®s!");
        Serial.println("Attente de donn√©es capteurs...");
        
        return true;
    }}

    void add_sensor_data(float* sensor_values) {{
        // Ajout des nouvelles donn√©es √† la s√©quence
        for (int i = 0; i < eps_guardian::ai_model::FEATURE_COUNT; i++) {{
            sensor_sequence[sequence_index][i] = sensor_values[i];
        }}
        
        sequence_index = (sequence_index + 1) % eps_guardian::ai_model::SEQUENCE_LENGTH;
    }}

    float detect_anomaly() {{
        // V√©rification que la s√©quence est compl√®te
        if (sequence_index != 0) {{
            Serial.println("ATTENTION: S√©quence incompl√®te, utilisation des donn√©es disponibles");
        }}
        
        // Copie des donn√©es dans le tenseur d'entr√©e
        float* input_data = input->data.f;
        for (int i = 0; i < eps_guardian::ai_model::SEQUENCE_LENGTH; i++) {{
            for (int j = 0; j < eps_guardian::ai_model::FEATURE_COUNT; j++) {{
                *input_data++ = sensor_sequence[i][j];
            }}
        }}
        
        // Inf√©rence
        if (interpreter->Invoke() != kTfLiteOk) {{
            Serial.println("ERREUR: Inf√©rence √©chou√©e!");
            return -1.0f;
        }}
        
        // Calcul de l'erreur de reconstruction
        float reconstruction_error = 0.0f;
        float* output_data = output->data.f;
        input_data = input->data.f;
        
        for (int i = 0; i < eps_guardian::ai_model::SEQUENCE_LENGTH * eps_guardian::ai_model::FEATURE_COUNT; i++) {{
            float diff = input_data[i] - output_data[i];
            reconstruction_error += diff * diff;
        }}
        
        reconstruction_error /= (eps_guardian::ai_model::SEQUENCE_LENGTH * eps_guardian::ai_model::FEATURE_COUNT);
        
        return reconstruction_error;
    }}

    int get_anomaly_level(float error) {{
        if (error < eps_guardian::ai_model::NORMAL_THRESHOLD) {{
            return 0; // NORMAL
        }} else if (error < eps_guardian::ai_model::WARNING_THRESHOLD) {{
            return 1; // WARNING
        }} else {{
            return 2; // CRITICAL
        }}
    }}

    void update_leds(int anomaly_level) {{
        // √âteindre toutes les LEDs
        digitalWrite(LED_NORMAL, LOW);
        digitalWrite(LED_WARNING, LOW);
        digitalWrite(LED_CRITICAL, LOW);
        
        // Allumer la LED correspondante
        switch (anomaly_level) {{
            case 0: // NORMAL
                digitalWrite(LED_NORMAL, HIGH);
                break;
            case 1: // WARNING
                digitalWrite(LED_WARNING, HIGH); 
                break;
            case 2: // CRITICAL
                digitalWrite(LED_CRITICAL, HIGH);
                break;
        }}
    }}

    void print_debug_info(float error, int level) {{
        Serial.print("Erreur reconstruction: ");
        Serial.print(error, 6);
        Serial.print(" | Niveau: ");
        
        switch (level) {{
            case 0: Serial.println("NORMAL"); break;
            case 1: Serial.println("WARNING"); break;
            case 2: Serial.println("CRITICAL"); break;
        }}
        
        Serial.print("Seuils - Normal<");
        Serial.print(eps_guardian::ai_model::NORMAL_THRESHOLD, 6);
        Serial.print(", Warning<");  
        Serial.print(eps_guardian::ai_model::WARNING_THRESHOLD, 6);
        Serial.print(", Critical>=");
        Serial.println(eps_guardian::ai_model::CRITICAL_THRESHOLD, 6);
    }}
}};

// ====================================================
// INSTANCE GLOBALE ET SETUP
// ====================================================

EPSGuardianAI guardianAI;

void setup() {{
    Serial.begin(115200);
    while (!Serial) {{ delay(10); }}
    
    Serial.println("================================================");
    Serial.println("EPS GUARDIAN - SYST√àME IA EMBARQU√â");
    Serial.println("D√©tection d'anomalies EPS en temps r√©el");
    Serial.println("================================================");
    
    if (!guardianAI.initialize()) {{
        Serial.println("√âCHEC: Initialisation IA - Arr√™t du syst√®me");
        while(1) {{ delay(1000); }}
    }}
}}

// ====================================================
// BOUCLE PRINCIPALE
// ====================================================

void loop() {{
    // SIMULATION: G√©n√©ration de donn√©es capteurs r√©alistes
    float sensor_data[eps_guardian::ai_model::FEATURE_COUNT];
    
    // Valeurs typiques d'un syst√®me EPS nominal
    sensor_data[0] = 7.4f + random(-10, 10) / 100.0f;  // V_batt
    sensor_data[1] = 1.2f + random(-20, 20) / 100.0f;  // I_batt  
    sensor_data[2] = 35.0f + random(-50, 50) / 10.0f;  // T_batt
    sensor_data[3] = 7.8f + random(-5, 5) / 100.0f;    // V_bus
    sensor_data[4] = 0.8f + random(-10, 10) / 100.0f;  // I_bus
    sensor_data[5] = 15.2f + random(-20, 20) / 10.0f;  // V_solar
    sensor_data[6] = 1.5f + random(-10, 10) / 100.0f;  // I_solar
    
    // Ajout des donn√©es √† la s√©quence
    guardianAI.add_sensor_data(sensor_data);
    
    // D√©tection d'anomalie
    float error = guardianAI.detect_anomaly();
    
    if (error >= 0) {{
        int anomaly_level = guardianAI.get_anomaly_level(error);
        guardianAI.update_leds(anomaly_level);
        guardianAI.print_debug_info(error, anomaly_level);
    }}
    
    // Cycle toutes les 2 secondes
    delay(2000);
}}
"""
            
            with open(sketch_path, "w") as f:
                f.write(sketch_content)
            
            logger.info(f"Sketch Arduino g√©n√©r√©: {sketch_path}")
            return True
            
        except Exception as e:
            logger.error(f"Erreur g√©n√©ration sketch: {e}")
            return False
    
    def copy_simple_model_to_mcu(self):
        """Copie √©galement le mod√®le simple pour le MCU de base"""
        try:
            # Copier le mod√®le quantifi√© depuis model_complex vers model_simple
            source_tflite = os.path.join(MODEL_DIR, "ai_model_lstm_autoencoder_quant.tflite")
            dest_tflite = os.path.join(MCU_MODEL_DIR, "ai_autoencoder.tflite")
            
            if os.path.exists(source_tflite):
                import shutil
                shutil.copy2(source_tflite, dest_tflite)
                logger.info(f"Mod√®le simple copi√© vers: {dest_tflite}")
                
                # Mettre √† jour les seuils pour le MCU simple
                mcu_thresholds = {
                    "thresholds": {
                        "normal": self.thresholds["normal_threshold"],
                        "warning": self.thresholds["warning_threshold"],
                        "critical": self.thresholds["critical_threshold"]
                    },
                    "conversion_date": datetime.now().isoformat(),
                    "source_model": "LSTM Autoencoder Complexe"
                }
                
                thresholds_path = os.path.join(MCU_MODEL_DIR, "ai_thresholds.json")
                with open(thresholds_path, 'w') as f:
                    json.dump(mcu_thresholds, f, indent=2)
                
                logger.info(f"Seuils mis √† jour pour MCU simple: {thresholds_path}")
                
            return True
            
        except Exception as e:
            logger.error(f"Erreur copie mod√®le MCU: {e}")
            return False
    
    def generate_deployment_report(self):
        """G√©n√®re un rapport de d√©ploiement complet"""
        report_path = os.path.join(DEPLOY_DIR, "deployment_report.md")
        
        report_content = f"""# Rapport de D√©ploiement IA - EPS Guardian

## Date de G√©n√©ration
{datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## Mod√®le Source
- **Architecture**: LSTM Autoencoder
- **Shape d'entr√©e**: {self.model_info['input_shape']}
- **Shape de sortie**: {self.model_info['output_shape']}
- **Param√®tres**: {self.model_info['total_params']:,}
- **Taille originale**: {self.model_info['model_size_h5']:.1f} KB

## Fichiers G√©n√©r√©s

### 1. Mod√®les TensorFlow Lite (dans model_complex)
- `ai_model_lstm_autoencoder.tflite` - Version standard
- `ai_model_lstm_autoencoder_quant.tflite` - Version quantifi√©e (recommand√©e)

### 2. Fichiers de D√©ploiement ESP32 (dans esp32_deployment)
- `eps_guardian_ai_model.h` - Header C++ avec mod√®le int√©gr√©
- `eps_guardian_inference.ino` - Sketch Arduino complet

### 3. Compatibilit√© MCU (dans model_simple)
- Mod√®le √©galement disponible pour MCU simple: `ai_autoencoder.tflite`

## Seuils d'Anomalie
- **NORMAL**: < {self.thresholds['normal_threshold']:.6f}
- **WARNING**: < {self.thresholds['warning_threshold']:.6f}  
- **CRITICAL**: >= {self.thresholds['critical_threshold']:.6f}

## Configuration Mat√©rielle Recommand√©e
- **MCU**: ESP32 (avec 4MB Flash)
- **RAM minimale**: 40KB pour Tensor Arena
- **Broches LEDs**: 2 (NORMAL), 4 (WARNING), 5 (CRITICAL)

## Utilisation
1. Inclure `eps_guardian_ai_model.h` dans votre projet
2. Utiliser la classe `EPSGuardianAI` pour l'inf√©rence
3. Appeler `add_sensor_data()` et `detect_anomaly()` cycliquement

## Performances Attendues
- **Temps d'inf√©rence**: < 100ms sur ESP32
- **Pr√©cision**: D√©tection d'anomalies temporelles complexes
- **Consommation**: Optimis√©e pour syst√®mes embarqu√©s

---
*G√©n√©r√© automatiquement par le syst√®me EPS Guardian*
"""
        
        with open(report_path, "w") as f:
            f.write(report_content)
        
        logger.info(f"Rapport de d√©ploiement g√©n√©r√©: {report_path}")

def main():
    """Point d'entr√©e principal"""
    logger.info("D√âMARRAGE CONVERSION MOD√àLE IA POUR ESP32")
    logger.info("=" * 60)
    
    converter = AIModelConverter()
    
    # 1. Chargement du mod√®le complexe
    if not converter.load_complex_model():
        logger.error("√âchec chargement mod√®le - Arr√™t")
        return 1
    
    # 2. Conversion en TFLite - SAUVEGARDE DANS MODEL_COMPLEX
    logger.info("CONVERSION EN TENSORFLOW LITE")
    
    # Version standard - dans model_complex
    tflite_standard = os.path.join(MODEL_DIR, "ai_model_lstm_autoencoder.tflite")
    size_standard = converter.convert_to_tflite(tflite_standard, quantize=False)
    
    # Version quantifi√©e - dans model_complex
    tflite_quant = os.path.join(MODEL_DIR, "ai_model_lstm_autoencoder_quant.tflite")
    size_quant = converter.convert_to_tflite(tflite_quant, quantize=True, quantization_type="float16")
    
    logger.info(f"R√©duction taille: {converter.model_info['model_size_h5']:.1f}KB ‚Üí {size_quant:.1f}KB " 
                f"({size_quant/converter.model_info['model_size_h5']*100:.1f}%)")
    
    # 3. G√©n√©ration des fichiers de d√©ploiement - dans esp32_deployment
    logger.info("G√âN√âRATION FICHIERS D√âPLOIEMENT")
    
    # Utiliser le mod√®le quantifi√© depuis model_complex pour g√©n√©rer le header
    header_path = os.path.join(DEPLOY_DIR, "eps_guardian_ai_model.h")
    converter.generate_c_header(tflite_quant, header_path)
    
    sketch_path = os.path.join(DEPLOY_DIR, "eps_guardian_inference.ino")
    converter.generate_arduino_sketch(sketch_path)
    
    # 4. Copie pour MCU simple - depuis model_complex vers model_simple
    converter.copy_simple_model_to_mcu()
    
    # 5. Rapport final
    converter.generate_deployment_report()
    
    logger.info("=" * 60)
    logger.info(" CONVERSION TERMIN√âE AVEC SUCC√àS!")
    logger.info(f" Dossier model_complex: {MODEL_DIR}")
    logger.info(" Fichiers g√©n√©r√©s dans model_complex:")
    logger.info(f"   - {tflite_standard}")
    logger.info(f"   - {tflite_quant} (recommand√©)")
    logger.info(f" Dossier d√©ploiement: {DEPLOY_DIR}")
    logger.info(" Fichiers de d√©ploiement:")
    logger.info(f"   - {header_path}")
    logger.info(f"   - {sketch_path}")
    logger.info(" Pr√™t pour d√©ploiement sur ESP32!")
    
    return 0

if __name__ == "__main__":
    exit(main())
